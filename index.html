<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.1">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.1" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.4.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Carlos Page">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Carlos Page">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Carlos Page">






  <link rel="canonical" href="http://yoursite.com/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Carlos Page</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Carlos Page</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">
    <a href="/schedule/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />Schedule</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/14/初识卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Carlos">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Carlos Page">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/14/初识卷积神经网络/" itemprop="url">
                  初识卷积神经网络
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-14 10:27:50 / Modified: 10:30:28" itemprop="dateCreated datePublished" datetime="2018-09-14T10:27:50+08:00">2018-09-14</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="卷积神经网络概述"><a href="#卷积神经网络概述" class="headerlink" title="卷积神经网络概述"></a>卷积神经网络概述</h1><p>卷积神经网络或CovNets,是一种空间上共享参数的神经网络。<br>假设有一张图片，包含高度、宽度和深度（RGB三色通道）。此时运行一个具有K个输出的小神经网络，如下图，这样把输出表示为垂直的一小列，，在不改变权重的情况下，把这个小神经网络滑过整个图片，就像刷墙一样水平垂直的滑动。<br><img src="https://i.loli.net/2018/03/24/5ab6516b8e780.png" alt="image"><br>这样滑完整个图片后，在输出端，就画出一副新的图片，如下图，它的宽度和高度与输入图片不同，更重要的是，它的深度跟之前不同，下现在不只是RGB三色通道,而是得到了K个颜色通道，这种操作叫做卷积。<br><img src="https://i.loli.net/2018/03/24/5ab652c47461b.png" alt="image"><br>如果你的块(patch)和整张图片一样大，那就和普通的神经网络没有区别。如下图：<img src="https://i.loli.net/2018/03/24/5ab6537dc971c.png" alt="image"><br>正是由于使用了小块，所以在整个空间共享较少的权重，卷积网络基本上是一个深度网络，但使用了共享权重的”卷积层”替代了一般的”全连接层”。<br>总的想法是让他们形成金字塔状，如下图所示，金字塔底部是一个非常大而浅的图片，仅包括红绿蓝三通道，通过卷积操作逐渐挤压空间的维度，同时不断增加深度，使深度信息大体上可表示出复杂的语义；在金字塔顶端，是一个分类器(classifier)，所有空间信息都被压缩成一个表示仅映射到图片内容的参数被保留。<br><img src="https://i.loli.net/2018/03/24/5ab654f74c7eb.png" alt="image"><br>如果想要实现，必须正确实现很多细节，还要习惯一些神经网络语言，像块(patch)和深度(depth),块有时也叫做核(Kernel)，堆栈中的每张图片都叫做特征图，如下图：<br><img src="https://i.loli.net/2018/03/24/5ab657509e6f9.png" alt="image"><br>这里，将三个特征图映射到K特征图。<br>还有一个术语叫做步幅(stride)，他是当移动过滤器时平移的像素的数量，步幅为1时输出的尺寸和输入大体相同，步幅为2时，尺寸为一半。如果移动从不超越边界，这种通常称为有效填充(vaild padding)。如果在边界外使用0填充，这样就会得到和输入图相同大小的输出图，这通常称为相同填充(same padding)。  </p>
<p>广义来说，CNN的学习方式是：他识别基本的直线，曲线，然后是形状，点块，然后是图片中更复杂的物体。最终CNN分类器把这些大的，复杂的物体综合起来识别图片。<br>有了深度学习，就不需要设定CNN来识别特定的特征。相反，CNN通过正向和反向传播，自己学习识别物体，尽管从没有让CNN寻找特定的特征信息，但是它识别图片的能力却好的惊人。<br>CNN可能有几层网络，每个层可能捕获对象抽象层次中的不同级别。第一层是抽象层次的最底级，CNN一般把图片中的较小的部分识别成简单的形状，例如水平、竖直的直线，简单的色块。下一层将会上升到更高的抽象层次，一般会识别更复杂的概念，例如形状(线的组合)，以此类推直至最终识别整个物体。<br>需要强调的是，CNN是自主学习。并不需要告诉CNN去寻找任何直线、曲线、鼻子、毛发等，CNN从训练集中学习并发现被识别物体值得寻找的特征。  </p>
<h2 id="分解一张图片"><a href="#分解一张图片" class="headerlink" title="分解一张图片"></a>分解一张图片</h2><p>CNN 的第一步是把图片分成小块，通过选取一个给定宽度和高度的滤波器来实现这一步。滤波器会照在图片的小块 patch （图像区块）上。这些 patch 的大小与滤波器一样大。如下图：<br><img src="https://i.loli.net/2018/03/24/5ab65beb1f5ce.png" alt="image"><br>可以在水平方向，或者竖直方向滑动滤波器对图片的不同部分进行聚焦。<br>滤波器滑动的间隔被称作 stride（步长）。这是可以调节的一个超参数。增大 stride 值后，会减少每层总 patch 数量，因此也减小了模型大小。通常这也会降低图像精度。<br>如下详细例子，在这个放大的狗图片中，从红框开始，滤波器的高和宽决定了这个正方形的大小。<br><img src="https://i.loli.net/2018/03/24/5ab65c796b9d8.png" alt="image"><br>然后向右把方块移动一个给定的步长（这里是2），得到另一块 patch。<br><img src="https://i.loli.net/2018/03/24/5ab65cb834be8.png" alt="image"><br>这里最重要的是把相邻的像素聚在一起，把他们视作一个集合。   </p>
<blockquote>
<p>在普通非卷积的神经网络中，忽略了这种临近性。在普通网络中，把输入图片中的每一个像素与下一层的神经元相连。图片中相邻像素在一起是有原因的，并且有着特殊意义，但普通网络没有有效利用好这些信息。 </p>
</blockquote>
<p>要利用这种临近结构，CNN 就要学习如何分类临近模式，例如图片中的形状和物体。  </p>
<h2 id="滤波器深度-Filter-Depth"><a href="#滤波器深度-Filter-Depth" class="headerlink" title="滤波器深度(Filter Depth)"></a>滤波器深度(Filter Depth)</h2><p>通常都会有多于一个滤波器，不同滤波器提取一个 patch 的不同特性。例如，一个滤波器寻找特定颜色，另一个寻找特定物体的特定形状。卷积层滤波器的数量被称为<strong>滤波器深度</strong>。<br><img src="https://i.loli.net/2018/03/24/5ab65dbd7e47e.png" alt="image"><br>每个 patch 连接的神经元数量取决于滤波器的深度，如果深度是 k，把每个 patch 与下一层的 k 个神经元相连。这样下一层的高度就是 k，如下图所示。实际操作中，k是一个可以调节的超参数，大多数的 CNNs 倾向于选择相同的起始值。<br>将patch与多个神经元相连的作用在于，一个patch可以有多个有意义的，可供提取的点。一个patch连接多个神经元可以保证CNNs学会提取任何它觉得重要的特征。  </p>
<h2 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h2><p><img src="https://i.loli.net/2018/03/24/5ab66eb1d7a34.png" alt="image"><br>在CNN的一层中的patch中共享权重W，无论猫在图片中的哪个位置都可以找到。  一个给定的patch的分类，是由patch对应的权重和偏置项决定的。<br>如果想让左上角的猫和右下角的猫以同样的方式被识别，他们的权重和偏置项需要一样，这样他们才能以同一种方法识别。<br>这正是CNNs中做的，一个给定输出层学到的权重和偏置项会共享在输入层所有的patch里。注意，当增大滤波器的深度的时候，需要学习的权重和偏置项的数量也会增加，因为权重并没有共享在所有输出中的channel里。<br>共享参数还有一个好处是，如果不在所有的patch里用相同的权重，则必须对每一个patch和它对应的隐藏层神经元学习新的参数。这不利于规模化，特别对高清图片。因此，共享权重不仅可以保持平移不变，还给出一个更小，可以规模化的模型。  </p>
<p><img src="https://i.loli.net/2018/03/24/5ab671481eb27.png" alt="image"><br>假设现在有一个 5x5 网格 (如上图所示) 和一个尺寸为 3x3 stride值为 1 的滤波器(filter)。 下一层的 width 和 height 是多少呢？ 如图中所示，在水平和竖直方向都可以在3个不同的位置放置 patch， 下一层的维度即为 3x3 。下一层宽和高的尺寸就会按此规则缩放。<br>在理想状态下，我们可以在层间保持相同的宽度和高度，以便继续添加图层，保持网络的一致性，而不用担心维度的缩小。如何实现这一构想？其中一种简单的办法是，在 5x5 原始图片的外层包裹一圈 0 ，如下图所示。<br><img src="https://i.loli.net/2018/03/24/5ab6721fb843d.png" alt="image"><br>这将会把原始图片扩展到 7x7。 现在我们知道如何让下一层图片的尺寸维持在 5x5，保持维度的一致性。  </p>
<h2 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h2><p>维度的计算方法：<br>输入层(input layer)维度值是W， 滤波器(filter)的维度值是F(height <em> width </em> depth), 步幅(stride)的数值是S， padding的数值是P，下一层的维度值可用如下公式表示：  </p>
<blockquote>
<p>(W - F + 2P)/S + 1 (分别计算高度和宽度)<br><strong>新的深度就是滤波器的数量</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; new_height = (input_height - filter_height + <span class="number">2</span> * P)/S + <span class="number">1</span></span><br><span class="line">&gt; new_width = (input_width - filter_width + <span class="number">2</span> * P)/S + <span class="number">1</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>例题：<br>H = height, W = width, D = depth</p>
<ul>
<li>我们有一个输入维度是 32x32x3 (HxWxD)</li>
<li>20个维度为 8x8x3 (HxWxD) 的滤波器</li>
<li>高和宽的stride（步长）都为 2。(S)</li>
<li>padding 大小为1 (P)<br>计算维度大小。<br>代码如下：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input = tf.placeholder(tf.float32, (<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>,<span class="number">3</span>))</span><br><span class="line">filter_weights = tf.Variable(tf.truncated_normal((<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, <span class="number">20</span>)))  <span class="comment"># (height,width,input_depth,output_depth)</span></span><br><span class="line">filter_bias = tf.Variable(tf.zeros(<span class="number">20</span>))</span><br><span class="line">strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>] <span class="comment">#(batch, height, width, depth)</span></span><br><span class="line">padding = <span class="string">'VALID'</span></span><br><span class="line">conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>注意，这里的<code>conv</code> 输出的是 [1, 13, 13, 20]。这是对应 batch size 的 4D 大小，重要的是它不是 [1, 14, 14, 20]。这是因为 TensorFlow 的 padding 算法与上面的并不完全相同。一个可替换方案是把 <code>padding</code> 从 <code>&#39;VALID&#39;</code> 改为<code>&#39;SAME&#39;</code>，这样得到的结果是 [1, 16, 16, 20]。  </p>
<p>总之，TensorFlow 使用如下等式计算 SAME 、PADDING<br><strong>SAME Padding</strong>， 输出的高和宽，计算如下：<br>out_height = ceil(float(in_height) / float(strides1))<br>out_width = ceil(float(in_width) / float(strides[2]))<br><strong>VALID Padding</strong>， 输出的高和宽，计算如下：<br>out_height = ceil(float(in_height - filter_height + 1) / float(strides1))<br>out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))  </p>
<h2 id="参数数量"><a href="#参数数量" class="headerlink" title="参数数量"></a>参数数量</h2><p>能够计算神经网络里面的参数数量很有用，因为我们想控制神经网络使用的内存空间。<br>在没有参数共享的前提下，每个输出层的神经元必须连接到滤波器的每个神经元，此外，每个输出层的神经元必须连接到一个偏执神经元。这种情况下，以上面的例子来计算，卷积层的参数是：  </p>
<blockquote>
<p>(8 <em> 8 </em> 3 + 1) <em> (14 </em> 14 * 20) = 756560  </p>
</blockquote>
<p>如果输出层的每个神经元与其他同样通道的神经元共享参数，这是实际卷积层(tf.nn.conv2d())使用的参数数量，则卷积层的参数数量是：  </p>
<blockquote>
<p>(8 <em> 8 </em> 3 + 1) * 20 = 3860  </p>
</blockquote>
<h1 id="卷积神经网络的TensorFlow实现"><a href="#卷积神经网络的TensorFlow实现" class="headerlink" title="卷积神经网络的TensorFlow实现"></a>卷积神经网络的TensorFlow实现</h1><h2 id="Tensorflow卷积层"><a href="#Tensorflow卷积层" class="headerlink" title="Tensorflow卷积层"></a>Tensorflow卷积层</h2><p>TensorFlow提供了tf.nn.conv2d()和tf.nn.bias_add()函数来创建自己的卷积层。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">k_output = <span class="number">64</span> <span class="comment">#output depth</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Image properties</span></span><br><span class="line">image_width = <span class="number">10</span></span><br><span class="line">image_height = <span class="number">10</span></span><br><span class="line">color_channels = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convolution filter</span></span><br><span class="line">filter_size_width = <span class="number">5</span></span><br><span class="line">filter_size_height = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input/Image</span></span><br><span class="line">input = tf.placeholder(</span><br><span class="line">    tf.float32,</span><br><span class="line">    shape=[<span class="keyword">None</span>, image_height, image_width, color_channels])</span><br><span class="line"></span><br><span class="line"><span class="comment">#weight and bias</span></span><br><span class="line">weight = tf.Variable(tf.truncated_normal(</span><br><span class="line">    [filter_size_height, filter_size_width, color_channels, k_output]))</span><br><span class="line">bias = tf.Variable(tf.zeros(k_output))</span><br><span class="line"></span><br><span class="line"><span class="comment">#apply Convolution</span></span><br><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add bias</span></span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line"><span class="comment"># apply activation function</span></span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br></pre></td></tr></table></figure></p>
<p>上述代码用了<code>tf.nn.conv2d()</code>函数来计算卷积，<code>weights</code>作为滤波器，<code>[1, 2, 2, 1]</code>作为strides。TensorFlow 对每一个<code>input</code>维度使用一个单独的 stride 参数，<code>[batch, input_height, input_width, input_channels]</code>。我们通常把<code>batch</code>和<code>input_channels</code>(<code>strides</code> 序列中的第一个第四个)的 stride 设为 1。<br>可以专注于修改<code>input_height</code>和<code>input_width</code>，<code>batch</code>和<code>input_channels</code> 都设置成 1。<code>input_height</code> 和 <code>input_width</code> strides 表示滤波器在<code>input</code>上移动的步长。上述例子中，在<code>input</code>之后，设置了一个 5x5 ，stride 为 2 的滤波器。<br>tf.nn.bias_add()函数对矩阵的走后一维加了偏置项。  </p>
<h2 id="TensorFlow最大池化"><a href="#TensorFlow最大池化" class="headerlink" title="TensorFlow最大池化"></a>TensorFlow最大池化</h2><p><img src="https://i.loli.net/2018/03/25/5ab6ffca88cbb.png" alt="image"><br>这是一个最大池化的例子,max pooling 用了 2x2 的滤波器 stride 为 2。四个 2x2 的颜色代表滤波器移动每个步长所产出的最大值。<br>例如<code>[[1, 0], [4, 6]]</code>生成 6，因为 6 是这4个数字中最大的。同理 <code>[[2, 3], [6, 8]]</code> 生成 8。 理论上，最大池化操作的好处是减小输入大小，使得神经网络能够专注于最重要的元素。最大池化只取覆盖区域中的最大值，其它的值都丢弃。降低过拟合是减小输出大小的结果，它同样也减少了后续层中的参数的数量。<br>TensorFlow提供了<code>tf.nn.max_pool()</code>函数，用于对卷积层实现最大池化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv_layer = tf.nn.conv2d(input, weight, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">conv_layer = tf.nn.bias_add(conv_layer, bias)</span><br><span class="line">conv_layer = tf.nn.relu(conv_layer)</span><br><span class="line"><span class="comment"># Apply Max Pooling</span></span><br><span class="line">conv_layer = tf.nn.max_pool(</span><br><span class="line">    conv_layer,</span><br><span class="line">    ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">    padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></p>
<p><code>tf.nn.max_pool()</code>函数实现最大池化时,<code>ksize</code>参数是滤波器大小，<code>strides</code>参数是步长。2x2 的滤波器配合 2x2 的步长是常用设定。<br><code>ksize</code>和<code>strides</code>参数也被构建为四个元素的列表，每个元素对应 input tensor 的一个维度 (<code>[batch, height, width, channels]</code>)，对<code>ksize</code> 和 <code>strides</code> 来说，batch 和 channel 通常都设置成 1。  </p>
<p>近期，池化层并不是很受青睐。部分原因是：</p>
<ul>
<li>现在的数据集又大又复杂，我们更关心欠拟合问题。</li>
<li>Dropout 是一个更好的正则化方法。</li>
<li>池化导致信息损失。想想最大池化的例子，n 个数字中我们只保留最大的，把余下的 n-1 完全舍弃了。  <h3 id="池化机制-维度大小"><a href="#池化机制-维度大小" class="headerlink" title="池化机制(维度大小)"></a>池化机制(维度大小)</h3>H = height, W = width, D = depth</li>
<li>输入维度是 4x4x5 (HxWxD)</li>
<li>滤波器大小 2x2 (HxW)</li>
<li>stride 的高和宽都是 2 (S)  </li>
</ul>
<p>新的高和宽的公式是：</p>
<blockquote>
<p>new_height = (input_height - filter_height)/S + 1<br>new_width = (input_width - filter_width)/S + 1</p>
</blockquote>
<p>结果是2<em>2</em>5</p>
<p><strong>注意</strong>：池化层的输出深度与输入的深度相同。另外池化操作是分别应用到每一个深度切片层。<br>下图是最大池化层工作原理示例。最大池化滤波器的大小是 2x2。当最大池化层在输入层滑动时，输出是这个 2x2 方块的最大值。<br><img src="https://i.loli.net/2018/03/25/5ab7031e30dd3.png" alt="image"><br>对应的代码是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input = tf.placeholder(tf.float32, (<span class="keyword">None</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line">filter_shape = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">padding = <span class="string">'VALID'</span></span><br><span class="line">pool = tf.nn.max_pool(input, filter_shape, strides, padding)</span><br></pre></td></tr></table></figure></p>
<p>pool 的输出维度是<code>[1, 2, 2, 5]</code>，即使把 padding 改成<code>&#39;SAME&#39;</code>也是一样。  </p>
<h2 id="Inception模块"><a href="#Inception模块" class="headerlink" title="Inception模块"></a>Inception模块</h2><p>Inception模块是这样的，它不局限于单个卷积运算，而是将多个模块组合，如平均池化后接1<em> 1的卷积，单独的1</em> 1的卷积接着3<em> 3的卷积，另一个1</em> 1的卷积接着5* 5的卷积；最后把这些运算的输出连成一串。如下图演示，这种方式虽然看上去很复杂，但是，根据所选择参数的方式，模型中的参数总数可能非常少，但模型的性能比使用简单卷积时要好。<br><img src="https://i.loli.net/2018/03/25/5ab7073732123.png" alt="image">  </p>
<h2 id="TensorFlow中的卷积网络实例"><a href="#TensorFlow中的卷积网络实例" class="headerlink" title="TensorFlow中的卷积网络实例"></a>TensorFlow中的卷积网络实例</h2><p>网络的结构跟经典的 CNNs 结构一样，是卷积层，最大池化层和全链接层的混合。  </p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>导入 MNIST 数据集，用一个方便的函数完成对数据集的 batch，缩放和独热编码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"."</span>, one_hot=<span class="keyword">True</span>, reshape=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">learning_rate = <span class="number">0.00001</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Number of samples to calculate validation and accuracy</span></span><br><span class="line"><span class="comment"># Decrease this if you're running out of memory to calculate accuracy</span></span><br><span class="line"><span class="comment"># 用来验证和计算准确率的样本数</span></span><br><span class="line"><span class="comment"># 如果内存不够，可以调小这个数字</span></span><br><span class="line">test_valid_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Network Parameters</span></span><br><span class="line"><span class="comment"># 神经网络参数</span></span><br><span class="line">n_classes = <span class="number">10</span>  <span class="comment"># MNIST total classes (0-9 digits)</span></span><br><span class="line">dropout = <span class="number">0.75</span>  <span class="comment"># Dropout, probability to keep units</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Weights-and-biases"><a href="#Weights-and-biases" class="headerlink" title="Weights and biases"></a>Weights and biases</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Store layers weight &amp; bias</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'wc1'</span>: tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])),</span><br><span class="line">    <span class="string">'wc2'</span>: tf.Variable(tf.random_normal([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])),</span><br><span class="line">    <span class="string">'wd1'</span>: tf.Variable(tf.random_normal([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>, n_classes]))&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'bc1'</span>: tf.Variable(tf.random_normal([<span class="number">32</span>])),</span><br><span class="line">    <span class="string">'bc2'</span>: tf.Variable(tf.random_normal([<span class="number">64</span>])),</span><br><span class="line">    <span class="string">'bd1'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))&#125;</span><br></pre></td></tr></table></figure>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p><img src="https://i.loli.net/2018/03/25/5ab709e0b406c.gif" alt="image"><br>这是一个 3x3 的卷积滤波器的示例。以 stride 为 1 应用到一个范围在 0 到 1 之间的数据上。每一个 3x3 的部分与权值<code>[[1, 0, 1], [0, 1, 0], [1, 0, 1]]</code>做卷积，把偏置加上后得到右边的卷积特征。这里偏置是 0 。TensorFlow 中这是通过<code>tf.nn.conv2d()</code>和<code>tf.nn.bias_add()</code>来完成的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, b, strides=<span class="number">1</span>)</span>:</span></span><br><span class="line">    x = tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, strides, strides, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    x = tf.nn.bias_add(x, b)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(x)</span><br></pre></td></tr></table></figure></p>
<p><code>tf.nn.conv2d()</code>函数与权值 W 做卷积。<br>在 TensorFlow 中，<code>strides</code> 是一个4个元素的序列；第一个位置表示 stride 的 batch 参数，最后一个位置表示 stride 的特征(feature)参数。最好的移除 batch 和特征(feature)的方法是你直接在数据集中把他们忽略，而不是使用 stride。要使用所有的 batch 和特征(feature)，你可以把第一个和最后一个元素设成1。<br>中间两个元素指纵向(height)和横向(width)的 stride，stride 通常是正方形，height = width。当别人说 stride 是 3 的时候，他们意思是 <code>tf.nn.conv2d(x, W, strides=[1, 3, 3, 1])</code>。<br>为了更简洁，这里的代码用了<code>tf.nn.bias_add()</code>来添加偏置。<code>tf.add()</code>这里不能使用，因为 tensors 的维度不同。  </p>
<h3 id="最大池化"><a href="#最大池化" class="headerlink" title="最大池化"></a>最大池化</h3><p><img src="https://i.loli.net/2018/03/25/5ab70c5acbe82.png" alt="image"><br>上面是一个最大池化的示例。滤波器大小是 2x2，stride 是 2。左边是输入，右边是输出。 四个 2x2 的颜色代表每一次滤波器应用在左侧来构建右侧的最大结果。例如<code>[[1, 1], [5, 6]]</code>变成 6，<code>[[3, 2], [1, 2]]</code>变成 3<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxpool2d</span><span class="params">(x, k=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(</span><br><span class="line">        x,</span><br><span class="line">        ksize=[<span class="number">1</span>, k, k, <span class="number">1</span>],</span><br><span class="line">        strides=[<span class="number">1</span>, k, k, <span class="number">1</span>],</span><br><span class="line">        padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure></p>
<p><code>tf.nn.max_pool()</code>函数通过设定 ksize 参数来设定滤波器大小，从而实现最大池化。  </p>
<h3 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h3><p>在下面的代码中，创建了 3 层来实现卷积，最大池化以及全链接层和输出层。每一层对维度的改变都写在注释里。例如第一层在卷积部分把图片从 28x28x1 变成了 28x28x32。后面应用了最大池化，每个样本变成了 14x14x32。从 conv1 经过多层网络，最后到 output 生成 10 个分类。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_net</span><span class="params">(x, weights, biases, dropout)</span>:</span></span><br><span class="line">    <span class="comment"># Layer 1 - 28*28*1 to 14*14*32</span></span><br><span class="line">    conv1 = conv2d(x, weights[<span class="string">'wc1'</span>], biases[<span class="string">'bc1'</span>])</span><br><span class="line">    conv1 = maxpool2d(conv1, k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Layer 2 - 14*14*32 to 7*7*64</span></span><br><span class="line">    conv2 = conv2d(conv1, weights[<span class="string">'wc2'</span>], biases[<span class="string">'bc2'</span>])</span><br><span class="line">    conv2 = maxpool2d(conv2, k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fully connected layer - 7*7*64 to 1024</span></span><br><span class="line">    fc1 = tf.reshape(conv2, [<span class="number">-1</span>, weights[<span class="string">'wd1'</span>].get_shape().as_list()[<span class="number">0</span>]])</span><br><span class="line">    fc1 = tf.add(tf.matmul(fc1, weights[<span class="string">'wd1'</span>]), biases[<span class="string">'bd1'</span>])</span><br><span class="line">    fc1 = tf.nn.relu(fc1)</span><br><span class="line">    fc1 = tf.nn.dropout(fc1, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output Layer - class prediction - 1024 to 10</span></span><br><span class="line">    out = tf.add(tf.matmul(fc1, weights[<span class="string">'out'</span>]), biases[<span class="string">'out'</span>])</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<h3 id="Session-运行"><a href="#Session-运行" class="headerlink" title="Session(运行)"></a>Session(运行)</h3><p>运行神经网络<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tf Graph input</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model</span></span><br><span class="line">logits = conv_net(x, weights, biases, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">cost = tf.reduce_mean(\</span><br><span class="line">    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\</span><br><span class="line">    .minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(logits, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initializing the variables</span></span><br><span class="line">init = tf. global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Launch the graph</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(mnist.train.num_examples//batch_size):</span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(optimizer, feed_dict=&#123;</span><br><span class="line">                x: batch_x,</span><br><span class="line">                y: batch_y,</span><br><span class="line">                keep_prob: dropout&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Calculate batch loss and accuracy</span></span><br><span class="line">            loss = sess.run(cost, feed_dict=&#123;</span><br><span class="line">                x: batch_x,</span><br><span class="line">                y: batch_y,</span><br><span class="line">                keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            valid_acc = sess.run(accuracy, feed_dict=&#123;</span><br><span class="line">                x: mnist.validation.images[:test_valid_size],</span><br><span class="line">                y: mnist.validation.labels[:test_valid_size],</span><br><span class="line">                keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">'Epoch &#123;:&gt;2&#125;, Batch &#123;:&gt;3&#125; -'</span></span><br><span class="line">                  <span class="string">'Loss: &#123;:&gt;10.4f&#125; Validation Accuracy: &#123;:.6f&#125;'</span>.format(</span><br><span class="line">                epoch + <span class="number">1</span>,</span><br><span class="line">                batch + <span class="number">1</span>,</span><br><span class="line">                loss,</span><br><span class="line">                valid_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate Test Accuracy</span></span><br><span class="line">    test_acc = sess.run(accuracy, feed_dict=&#123;</span><br><span class="line">        x: mnist.test.images[:test_valid_size],</span><br><span class="line">        y: mnist.test.labels[:test_valid_size],</span><br><span class="line">        keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">    print(<span class="string">'Testing Accuracy: &#123;&#125;'</span>.format(test_acc))</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/12/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Carlos">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Carlos Page">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/12/hello-world/" itemprop="url">
                  Hello World
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-09-12 23:13:17" itemprop="dateCreated datePublished" datetime="2018-09-12T23:13:17+08:00">2018-09-12</time>
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Carlos" />
            
              <p class="site-author-name" itemprop="name">Carlos</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Carlosnight" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:yeyiyajun@live.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Carlos</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.4.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.4.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  










  





  

  

  

  

  
  

  

  

  

  

  

</body>
</html>
